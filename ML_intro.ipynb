{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0d6f3ed-f80c-425f-ba5f-f8f8df046e12",
   "metadata": {},
   "source": [
    "# ML for biological data analysis - practical examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db250b3b-2d4d-459b-b2e8-f6a35fc7815e",
   "metadata": {},
   "source": [
    "## 1. Image classification using convolutional neural networks\n",
    "We will use images of micropatterned organoids with healthy (WT) and diseased background (Huntington's disease, HD), from [Metzger et al. 2022](https://www.sciencedirect.com/science/article/pii/S2667237522001795?via%3Dihub). We will use a much reduced dataset (about 100 images per condition) and downsampled images to increase computational speed. Usually, datasets for deep learning are much larger. This first part is mostly an adaptation of [PyTorch's Transfer Learning for Computer Vision Tutorial](https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554be31e-4ea0-4866-a6fb-7a38922ad8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch \n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.backends.cudnn as cudnn\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "from tempfile import TemporaryDirectory\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import umap.umap_ as umap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d211c8-4412-4e16-9217-a44fd932e76f",
   "metadata": {},
   "source": [
    "Prepare training ('train') and validation ('val') sets, and prepare data augmentation for training. Here, we will not do data augementation for validation. Are there cases where it could make sense to use data augmentation also for validation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b710c0a4-0248-46f2-b77d-c55d1d26fbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation and normalization for training\n",
    "# Just normalization for validation\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "data_dir = 'data/'\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                                          data_transforms[x])\n",
    "                  for x in ['train', 'val']}\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4,\n",
    "                                             shuffle=True, num_workers=4)\n",
    "              for x in ['train', 'val']}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
    "class_names = image_datasets['train'].classes\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b77e1a-6a42-475a-9fc6-4189bc1c97f2",
   "metadata": {},
   "source": [
    "Display some example images for the two conditions, WT and HD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fb6ae0-5c2b-4ada-9be7-f0de50b498a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(inp, title=None):\n",
    "    \"\"\"Display image for Tensor.\"\"\"\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    plt.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "\n",
    "\n",
    "# Get a batch of training data\n",
    "inputs, classes = next(iter(dataloaders['train']))\n",
    "\n",
    "# Make a grid from batch\n",
    "out = torchvision.utils.make_grid(inputs)\n",
    "\n",
    "imshow(out, title=[class_names[x] for x in classes])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a2d6d0-cbba-427b-bc2b-c6e9607a01ca",
   "metadata": {},
   "source": [
    "Definde the training function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04678d98-d97b-4e63-95f0-feea95ee5906",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    # Create a temporary directory to save training checkpoints\n",
    "    with TemporaryDirectory() as tempdir:\n",
    "        best_model_params_path = os.path.join(tempdir, 'best_model_params.pt')\n",
    "\n",
    "        torch.save(model.state_dict(), best_model_params_path)\n",
    "        best_acc = 0.0\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "            print('-' * 10)\n",
    "\n",
    "            # Each epoch has a training and validation phase\n",
    "            for phase in ['train', 'val']:\n",
    "                if phase == 'train':\n",
    "                    model.train()  # Set model to training mode\n",
    "                else:\n",
    "                    model.eval()   # Set model to evaluate mode\n",
    "\n",
    "                running_loss = 0.0\n",
    "                running_corrects = 0\n",
    "\n",
    "                # Iterate over data.\n",
    "                for inputs, labels in dataloaders[phase]:\n",
    "                    inputs = inputs.to(device)\n",
    "                    labels = labels.to(device)\n",
    "\n",
    "                    # zero the parameter gradients\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    # forward\n",
    "                    # track history if only in train\n",
    "                    with torch.set_grad_enabled(phase == 'train'):\n",
    "                        outputs = model(inputs)\n",
    "                        _, preds = torch.max(outputs, 1)\n",
    "                        loss = criterion(outputs, labels)\n",
    "\n",
    "                        # backward + optimize only if in training phase\n",
    "                        if phase == 'train':\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "\n",
    "                    # statistics\n",
    "                    running_loss += loss.item() * inputs.size(0)\n",
    "                    running_corrects += torch.sum(preds == labels.data)\n",
    "                if phase == 'train':\n",
    "                    scheduler.step()\n",
    "\n",
    "                epoch_loss = running_loss / dataset_sizes[phase]\n",
    "                epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "                print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "                # deep copy the model\n",
    "                if phase == 'val' and epoch_acc > best_acc:\n",
    "                    best_acc = epoch_acc\n",
    "                    torch.save(model.state_dict(), best_model_params_path)\n",
    "\n",
    "            print()\n",
    "\n",
    "        time_elapsed = time.time() - since\n",
    "        print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "        print(f'Best val Acc: {best_acc:4f}')\n",
    "\n",
    "        # load best model weights\n",
    "        model.load_state_dict(torch.load(best_model_params_path, weights_only=True))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfdc7d2-5de9-449d-930c-05fa2e5faefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_model(model, num_images=6):\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "    images_so_far = 0\n",
    "    fig = plt.figure()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, labels) in enumerate(dataloaders['val']):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            for j in range(inputs.size()[0]):\n",
    "                images_so_far += 1\n",
    "                ax = plt.subplot(num_images//2, 2, images_so_far)\n",
    "                ax.axis('off')\n",
    "\n",
    "                # Set title to show both predicted and true labels\n",
    "                ax.set_title(f'predicted: {class_names[preds[j]]}\\ntrue: {class_names[labels[j]]}')\n",
    "                imshow(inputs.cpu().data[j])\n",
    "\n",
    "                if images_so_far == num_images:\n",
    "                    model.train(mode=was_training)\n",
    "                    return\n",
    "        model.train(mode=was_training)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6baa6c-0d89-4bd4-9002-e938115b17c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft = models.resnet18(weights='IMAGENET1K_V1')\n",
    "num_ftrs = model_ft.fc.in_features\n",
    "\n",
    "model_ft.fc = nn.Linear(num_ftrs, 2) # Here the size of each output sample is set to 2, because we have two conditions, WT and HD\n",
    "\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5b413d-abe0-4846-a8c3-4c6fe1b1836f",
   "metadata": {},
   "source": [
    "We can now run the model. How many epochs should we run it for?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57baefa-02f7-4355-b218-55e7cd7854ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=4) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c1826d-9e15-4725-aa7b-a29057dfded5",
   "metadata": {},
   "source": [
    "Visualize the predictions of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ffbf29-7486-49d7-aada-76f5f952a360",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_model(model_ft)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ec1551-4519-4ab1-a6a5-a8549a5db7fe",
   "metadata": {},
   "source": [
    "To evaluate the model performance, we can plot a confusion matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66b15c8-4dc1-47f1-9d46-aaec7360ad3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_and_collect_predictions(model, dataloaders):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloaders['val']:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            # Append predictions and true labels\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    return np.array(all_preds), np.array(all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ec98ff-77a1-411f-8737-5b1c42f8c5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, class_names):\n",
    "    # Generate confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    # Plot the confusion matrix\n",
    "    plt.figure(figsize=(4, 3))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='crest_r', xticklabels=class_names, yticklabels=class_names, linewidth=.5)\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.title('Confusion Matrix')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2cd30a-0ccf-486d-b565-1908720a3906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have your model, dataloaders, and class_names defined\n",
    "\n",
    "# Collect predictions and true labels\n",
    "preds, true_labels = evaluate_model_and_collect_predictions(model_ft, dataloaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3465cb4a-88a8-480e-b8ed-ce7df457990b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the confusion matrix\n",
    "plot_confusion_matrix(true_labels, preds, class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55bacade-2ce1-4141-a37d-32811a782ec9",
   "metadata": {},
   "source": [
    "What other evaluation metrics can you think of? Are you satisfied with the results? What could be improved?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50488afe-e0c7-4843-804c-ddcd97ad8fdf",
   "metadata": {},
   "source": [
    "## 2. Unsupervised analysis using PCA, UMAP and autoencoders\n",
    "We will now use unsupervised learning on the same images of micropatterned organoids. Why would we do this? What can we learn from this? What can we learn from this that we couldn't learn using the classification approach?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a200152d-193a-4cf4-a3d5-07238c3c00a2",
   "metadata": {},
   "source": [
    "Both PCA and UMAP operate on vectors (i.e. 1-dimensional data), not 2-dimensional data such as the images we have here. We therefore need to flatten the images to one long 1-D vector. What do we lose in this process? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2af1f6-dffe-49a1-9bbe-45f311882645",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_images(dataloader):\n",
    "    all_flattened_images = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloaders['val']:\n",
    "            # Move images to CPU and flatten them\n",
    "            flattened = inputs.view(inputs.size(0), -1).cpu().numpy()  # Flatten each image\n",
    "            all_flattened_images.append(flattened)\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "\n",
    "    # Concatenate all batches\n",
    "    all_flattened_images = np.concatenate(all_flattened_images, axis=0)\n",
    "    all_labels = np.concatenate(all_labels, axis=0)\n",
    "\n",
    "    return all_flattened_images, all_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fcf542-10f8-43ca-8501-869624a91010",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import umap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def apply_pca_umap(flattened_images, labels, class_names):\n",
    "    # Apply PCA\n",
    "    pca = PCA(n_components=2)\n",
    "    pca_result = pca.fit_transform(flattened_images)\n",
    "\n",
    "    # Apply UMAP\n",
    "    # reducer = umap.UMAP(random_state=42)\n",
    "\n",
    "    umap_result = umap.UMAP(n_components=2).fit_transform(flattened_images)\n",
    "\n",
    "    # Plot PCA result\n",
    "    plt.figure(figsize=(8, 4))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    for class_idx, class_name in enumerate(class_names):\n",
    "        plt.scatter(pca_result[labels == class_idx, 0], pca_result[labels == class_idx, 1], \n",
    "                    label=class_name, alpha=0.7)\n",
    "    plt.title('PCA')\n",
    "    plt.legend()\n",
    "    sns.despine()\n",
    "    \n",
    "    # Plot UMAP result\n",
    "    plt.subplot(1, 2, 2)\n",
    "    for class_idx, class_name in enumerate(class_names):\n",
    "        plt.scatter(umap_result[labels == class_idx, 0], umap_result[labels == class_idx, 1], \n",
    "                    label=class_name, alpha=0.7)\n",
    "    plt.title('UMAP')\n",
    "    plt.legend()\n",
    "    sns.despine()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618d6d80-5ddc-4846-874c-3d9dc980afe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Flatten the images\n",
    "flattened_images, image_labels = flatten_images(dataloaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff84b5b-d152-4d11-9870-afd249e5f85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Apply PCA and UMAP and visualize the result\n",
    "apply_pca_umap(flattened_images, image_labels, class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640c053b-5951-4ac5-920b-7c5d90eb088b",
   "metadata": {},
   "source": [
    "In the final step, we will use a convolutional autoencoder to analyze the images. Why would we do this? Do you expect this to work better or worse than PCA/UMAP? Carefully inspect the code for the autoencoder and check that you understand the basic structure. Why does the forward function return two outputs (encoded, decoded)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d5a48f-8440-4b5d-9b71-f844dbdee4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class ConvAutoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvAutoencoder, self).__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, 3, stride=2, padding=1),  # Input: (3, 64, 64), Output: (16, 32, 32)\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, 3, stride=2, padding=1), # Output: (32, 16, 16)\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 7)                      # Output: (64, 10, 10)\n",
    "        )\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(64, 32, 7),            # Output: (32, 16, 16)\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 16, 3, stride=2, padding=1, output_padding=1), # Output: (16, 32, 32)\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(16, 3, 3, stride=2, padding=1, output_padding=1),  # Output: (3, 64, 64)\n",
    "            nn.Sigmoid()  # Use Sigmoid if the images are normalized to [0, 1]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)  # Forward pass through the encoder\n",
    "        decoded = self.decoder(encoded)  # Forward pass through the decoder\n",
    "        return encoded, decoded  # Return both encoded (latent) and decoded (reconstructed) outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ff6e47-43e8-469c-8c31-0e0cf6b1b129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function and optimizer\n",
    "autoencoder = ConvAutoencoder().to(device)\n",
    "criterion = nn.MSELoss()  # Mean Squared Error for reconstruction\n",
    "optimizer = optim.Adam(autoencoder.parameters(), lr=1e-3)\n",
    "\n",
    "# Training function that returns the loss history\n",
    "def train_autoencoder(model, dataloader, num_epochs=20):\n",
    "    model.train()\n",
    "    train_losses = []  # List to store the loss after each epoch\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for data in dataloader:\n",
    "            inputs, _ = data\n",
    "            inputs = inputs.to(device)\n",
    "            \n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            encoded, decoded = model(inputs)\n",
    "            \n",
    "            # Compute the loss\n",
    "            loss = criterion(decoded, inputs)\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        # Compute the average loss for this epoch\n",
    "        avg_loss = running_loss / len(dataloader)\n",
    "        train_losses.append(avg_loss)  # Append the average loss to the list\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss}')\n",
    "    \n",
    "    # Return the loss history\n",
    "    return train_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3cbc08-2d2f-46ab-97ca-bbe808801a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "train_losses = train_autoencoder(autoencoder, dataloaders['train'], num_epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b303b32-35b8-43cc-8229-6f4b1828fddd",
   "metadata": {},
   "source": [
    "Let's plot the loss function (i.e. how well the network learns over the epchs). What do you observe? Are you satisfied with the results? How could they be improved?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882d8074-bb5a-4825-bbb2-d2fb83da6964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot function\n",
    "def plot_loss(train_losses):\n",
    "    plt.plot(range(1, len(train_losses) + 1), train_losses, label='Training Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')    \n",
    "    plt.legend()\n",
    "\n",
    "# Plot the loss\n",
    "plot_loss(train_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86d2d51-1742-48c8-8b81-92a632687dda",
   "metadata": {},
   "source": [
    "To cluster the results, we will get the latent space of the autoencoder for each images of the validation set. What is the latent space and why is it useful?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe437a1-2939-47fc-ae20-4bda7865a055",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latent_space(model, dataloader):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    latent_vectors = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.to(device)\n",
    "            encoded, _ = model(inputs)\n",
    "            \n",
    "            # Flatten the latent space for clustering (if needed)\n",
    "            latent_vectors.append(encoded.view(encoded.size(0), -1).cpu().numpy())\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "    \n",
    "    return np.concatenate(latent_vectors, axis=0), np.concatenate(all_labels, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6f3f81-ca6b-4e0b-a6eb-2e9e81c9eec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import umap.umap_ as umap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# PCA on Latent Space\n",
    "pca = PCA(n_components=2)\n",
    "pca_result = pca.fit_transform(latent_vectors)\n",
    "\n",
    "# UMAP on Latent Space\n",
    "umap_result = umap.UMAP(n_components=2).fit_transform(latent_vectors)\n",
    "\n",
    "# Plot PCA and UMAP results\n",
    "plt.figure(figsize=(8, 4))\n",
    "\n",
    "# PCA plot\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(pca_result[:, 0], pca_result[:, 1], c=true_labels, cmap='viridis', s=10)\n",
    "plt.title('PCA on Latent Space')\n",
    "sns.despine()\n",
    "\n",
    "# UMAP plot\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(umap_result[:, 0], umap_result[:, 1], c=true_labels, cmap='viridis', s=10)\n",
    "plt.title('UMAP on Latent Space')\n",
    "sns.despine()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745364e2-c017-4f6d-a753-a04c1361732b",
   "metadata": {},
   "source": [
    "How do the autoencoder results compare to PCA and UMAP done on the flattened images? What are your conclusions about the different approaches? Which one would you focus on going forward? How could this be improved further?\n",
    "\n",
    "How are the results from the second part (PCA, UMAP, autoencoder) different to the first (classifcation using a convolutional neural network)? Which approach would be useful for which scenario? Discuss!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54917876-ed2f-493c-af6f-e98d988347b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
